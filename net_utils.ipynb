{"cells":[{"cell_type":"code","execution_count":null,"id":"944f3ecc","metadata":{"id":"944f3ecc"},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":null,"id":"8016c686","metadata":{"id":"8016c686"},"outputs":[],"source":["def activation_function(activation_fn):\n","    '''\n","    Select activation function\n","\n","    Arg(s):\n","        activation_fn : str\n","            name of activation function\n","    Returns:\n","        torch.nn.Module : activation function\n","    '''\n","\n","    if 'linear' in activation_fn:\n","        return None\n","    elif 'leaky_relu' in activation_fn:\n","        return torch.nn.LeakyReLU(negative_slope=0.10, inplace=True)\n","    elif 'relu' in activation_fn:\n","        return torch.nn.ReLU()\n","    elif 'elu' in activation_fn:\n","        return torch.nn.ELU()\n","    elif 'sigmoid' in activation_fn:\n","        return torch.nn.Sigmoid()\n","    else:\n","        raise ValueError('Unsupported activation function: {}'.format(activation_fn))"]},{"cell_type":"code","execution_count":null,"id":"6e6c40ba","metadata":{"id":"6e6c40ba"},"outputs":[],"source":["'''\n","Network layers\n","'''\n","class Conv2d(torch.nn.Module):\n","    '''\n","    2D convolution class\n","\n","    Arg(s):\n","        in_channels : int\n","            number of input channels\n","        out_channels : int\n","            number of output channels\n","        kernel_size : int\n","            size of kernel\n","        stride : int\n","            stride of convolution\n","        weight_initializer : str\n","            kaiming_normal, kaiming_uniform, xavier_normal, xavier_uniform\n","        activation_func : func\n","            activation function after convolution\n","        use_batch_norm : bool\n","            if set, then applied batch normalization\n","        use_instance_norm : bool\n","            if set, then applied instance normalization\n","    '''\n","\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 kernel_size=3,\n","                 stride=1,\n","                 weight_initializer='kaiming_uniform',\n","                 activation_func=torch.nn.LeakyReLU(negative_slope=0.10, inplace=True),\n","                 use_batch_norm=False,\n","                 use_instance_norm=False):\n","        super(Conv2d, self).__init__()\n","\n","        padding = kernel_size // 2\n","\n","        self.conv = torch.nn.Conv2d(\n","            in_channels,\n","            out_channels,\n","            kernel_size=kernel_size,\n","            stride=stride,\n","            padding=padding,\n","            bias=False)\n","\n","        # Select the type of weight initialization, by default kaiming_uniform\n","        if weight_initializer == 'kaiming_normal':\n","            torch.nn.init.kaiming_normal_(self.conv.weight)\n","        elif weight_initializer == 'xavier_normal':\n","            torch.nn.init.xavier_normal_(self.conv.weight)\n","        elif weight_initializer == 'xavier_uniform':\n","            torch.nn.init.xavier_uniform_(self.conv.weight)\n","        elif weight_initializer == 'kaiming_uniform':\n","            pass\n","        else:\n","            raise ValueError('Unsupported weight initializer: {}'.format(weight_initializer))\n","\n","        self.activation_func = activation_func\n","\n","        assert not (use_batch_norm and use_instance_norm), \\\n","            'Unable to apply both batch and instance normalization'\n","\n","        self.use_norm = use_batch_norm or use_instance_norm\n","\n","        if use_batch_norm:\n","            self.norm = torch.nn.BatchNorm2d(out_channels)\n","        elif use_instance_norm:\n","            self.norm = torch.nn.InstanceNorm2d(out_channels)\n","\n","    def forward(self, x):\n","        '''\n","        Forward input x through a convolution layer\n","\n","        Arg(s):\n","            x : torch.Tensor[float32]\n","                N x C x H x W input tensor\n","        Returns:\n","            torch.Tensor[float32] : N x K x h x w output tensor\n","        '''\n","\n","        conv = self.conv(x)\n","        conv = self.norm(conv) if self.use_norm else conv\n","\n","        if self.activation_func is not None:\n","            return self.activation_func(conv)\n","        else:\n","            return conv"]},{"cell_type":"code","execution_count":null,"id":"9655ca8f","metadata":{"id":"9655ca8f"},"outputs":[],"source":["class TransposeConv2d(torch.nn.Module):\n","    '''\n","    Transpose convolution class\n","\n","    Arg(s):\n","        in_channels : int\n","            number of input channels\n","        out_channels : int\n","            number of output channels\n","        kernel_size : int\n","            size of kernel (k x k)\n","        weight_initializer : str\n","            kaiming_normal, kaiming_uniform, xavier_normal, xavier_uniform\n","        activation_func : func\n","            activation function after convolution\n","        use_batch_norm : bool\n","            if set, then applied batch normalization\n","        use_instance_norm : bool\n","            if set, then applied instance normalization\n","    '''\n","\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 kernel_size=3,\n","                 weight_initializer='kaiming_uniform',\n","                 activation_func=torch.nn.LeakyReLU(negative_slope=0.10, inplace=True),\n","                 use_batch_norm=False,\n","                 use_instance_norm=False):\n","        super(TransposeConv2d, self).__init__()\n","\n","        padding = kernel_size // 2\n","\n","        self.deconv = torch.nn.ConvTranspose2d(\n","            in_channels,\n","            out_channels,\n","            kernel_size=kernel_size,\n","            stride=2,\n","            padding=padding,\n","            output_padding=1,\n","            bias=False)\n","\n","        # Select the type of weight initialization, by default kaiming_uniform\n","        if weight_initializer == 'kaiming_normal':\n","            torch.nn.init.kaiming_normal_(self.deconv.weight)\n","        elif weight_initializer == 'xavier_normal':\n","            torch.nn.init.xavier_normal_(self.deconv.weight)\n","        elif weight_initializer == 'xavier_uniform':\n","            torch.nn.init.xavier_uniform_(self.deconv.weight)\n","        elif weight_initializer == 'kaiming_uniform':\n","            pass\n","        else:\n","            raise ValueError('Unsupported weight initializer: {}'.format(weight_initializer))\n","\n","        self.activation_func = activation_func\n","\n","        assert not (use_batch_norm and use_instance_norm), \\\n","            'Unable to apply both batch and instance normalization'\n","\n","        self.use_norm = use_batch_norm or use_instance_norm\n","\n","        if use_batch_norm:\n","            self.norm = torch.nn.BatchNorm2d(out_channels)\n","        elif use_instance_norm:\n","            self.norm = torch.nn.InstanceNorm2d(out_channels)\n","\n","    def forward(self, x):\n","        '''\n","        Forward input x through a transposed convolution layer\n","\n","        Arg(s):\n","            x : torch.Tensor[float32]\n","                N x C x h x w input tensor\n","        Returns:\n","            torch.Tensor[float32] : N x K x H x W output tensor\n","        '''\n","\n","        deconv = self.deconv(x)\n","        deconv = self.norm(deconv) if self.use_norm else deconv\n","\n","        if self.activation_func is not None:\n","            return self.activation_func(deconv)\n","        else:\n","            return deconv"]},{"cell_type":"code","execution_count":null,"id":"5d2b7093","metadata":{"id":"5d2b7093"},"outputs":[],"source":["class UpConv2d(torch.nn.Module):\n","    '''\n","    Up-convolution (upsample + convolution) block class\n","\n","    Arg(s):\n","        in_channels : int\n","            number of input channels\n","        out_channels : int\n","            number of output channels\n","        shape : list[int]\n","            two element tuple of ints (height, width)\n","        kernel_size : int\n","            size of kernel (k x k)\n","        weight_initializer : str\n","            kaiming_normal, kaiming_uniform, xavier_normal, xavier_uniform\n","        activation_func : func\n","            activation function after convolution\n","        use_batch_norm : bool\n","            if set, then applied batch normalization\n","        use_instance_norm : bool\n","            if set, then applied instance normalization\n","    '''\n","\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 kernel_size=3,\n","                 weight_initializer='kaiming_uniform',\n","                 activation_func=torch.nn.LeakyReLU(negative_slope=0.10, inplace=True),\n","                 use_batch_norm=False,\n","                 use_instance_norm=False):\n","        super(UpConv2d, self).__init__()\n","\n","        self.conv = Conv2d(\n","            in_channels,\n","            out_channels,\n","            kernel_size=kernel_size,\n","            stride=1,\n","            weight_initializer=weight_initializer,\n","            activation_func=activation_func,\n","            use_batch_norm=use_batch_norm,\n","            use_instance_norm=use_instance_norm)\n","\n","    def forward(self, x, shape):\n","        '''\n","        Forward input x through an up convolution layer\n","\n","        Arg(s):\n","            x : torch.Tensor[float32]\n","                N x C x h x w input tensor\n","            shape : tuple[int]\n","                height, width (H, W) tuple denoting output shape\n","        Returns:\n","            torch.Tensor[float32] : N x K x H x W output tensor\n","        '''\n","\n","        upsample = torch.nn.functional.interpolate(x, size=shape, mode='nearest')\n","        conv = self.conv(upsample)\n","        return conv"]},{"cell_type":"code","execution_count":null,"id":"b2399f88","metadata":{"lines_to_next_cell":1,"id":"b2399f88"},"outputs":[],"source":["'''\n","Network encoder blocks\n","'''\n","class ResNetBlock(torch.nn.Module):\n","    '''\n","    Basic ResNet block class\n","\n","    Arg(s):\n","        in_channels : int\n","            number of input channels\n","        out_channels : int\n","            number of output channels\n","        stride : int\n","            stride of convolution\n","        weight_initializer : str\n","            kaiming_normal, kaiming_uniform, xavier_normal, xavier_uniform\n","        activation_func : func\n","            activation function after convolution\n","        use_batch_norm : bool\n","            if set, then applied batch normalization\n","        use_instance_norm : bool\n","            if set, then applied instance normalization\n","    '''\n","\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 stride=1,\n","                 weight_initializer='kaiming_uniform',\n","                 activation_func=torch.nn.LeakyReLU(negative_slope=0.10, inplace=True),\n","                 use_batch_norm=False,\n","                 use_instance_norm=False):\n","        super(ResNetBlock, self).__init__()\n","\n","        self.activation_func = activation_func\n","\n","        # TODO: Implement ResNet block based on\n","        # Deep Residual Learning for Image Recognition: https://arxiv.org/pdf/1512.03385.pdf\n","\n","        self.conv1 = Conv2d(\n","            in_channels,\n","            out_channels,\n","            kernel_size=3,\n","            stride=stride,\n","            weight_initializer=weight_initializer,\n","            activation_func=activation_func,\n","            use_batch_norm=True,\n","        )\n","\n","        self.conv2 = Conv2d(\n","            out_channels,\n","            out_channels,\n","            kernel_size=3,\n","            weight_initializer=weight_initializer,\n","            activation_func=None,\n","            use_batch_norm=True,\n","        )\n","\n","        self.projection = Conv2d(\n","            in_channels,\n","            out_channels,\n","            kernel_size=1,\n","            stride=stride,\n","            weight_initializer=weight_initializer,\n","            activation_func=None,\n","            use_batch_norm=False,\n","            use_instance_norm=False\n","        )\n","\n","    def forward(self, x):\n","        '''\n","        Forward input x through a basic ResNet block\n","\n","        Arg(s):\n","            x : torch.Tensor[float32]\n","                N x C x H x W input tensor\n","        Returns:\n","            torch.Tensor[float32] : N x K x h x w output tensor\n","        '''\n","\n","        conv1 = self.conv1(x)\n","        conv2 = self.conv2(conv1)\n","\n","        if self.projection and conv2.size() != x.size():\n","          x = self.projection(x)\n","\n","        return self.activation_func(conv2 + x)"]},{"cell_type":"code","execution_count":null,"id":"5357fc5f","metadata":{"id":"5357fc5f"},"outputs":[],"source":["class VGGNetBlock(torch.nn.Module):\n","    '''\n","    VGGNet block class\n","\n","    Arg(s):\n","        in_channels : int\n","            number of input channels\n","        out_channels : int\n","            number of output channels\n","        n_convolution : int\n","            number of convolution layers\n","        stride : int\n","            stride of convolution\n","        weight_initializer : str\n","            kaiming_normal, kaiming_uniform, xavier_normal, xavier_uniform\n","        activation_func : func\n","            activation function after convolution\n","        use_batch_norm : bool\n","            if set, then applied batch normalization\n","        use_instance_norm : bool\n","            if set, then applied instance normalization\n","    '''\n","\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 n_convolution=1,\n","                 stride=1,\n","                 weight_initializer='kaiming_uniform',\n","                 activation_func=torch.nn.LeakyReLU(negative_slope=0.10, inplace=True),\n","                 use_batch_norm=False,\n","                 use_instance_norm=False):\n","        super(VGGNetBlock, self).__init__()\n","\n","        conv2d = Conv2d\n","\n","        layers = []\n","\n","        # TODO: Implement VGGNet architecture based on\n","        # Very Deep Convolutional Networks for Large-Scale Image Recognition: https://arxiv.org/pdf/1409.1556.pdf\n","        conv0 = conv2d(\n","            in_channels=out_channels,\n","            out_channels=out_channels,\n","            stride=stride,\n","            weight_initializer=weight_initializer,\n","            activation_func=activation_func,\n","            use_batch_norm=use_batch_norm,\n","            use_instance_norm=use_instance_norm\n","        )\n","        layers.append(conv0)\n","\n","        for i in range(n_convolution - 1):\n","            conv = conv2d(\n","                in_channels=in_channels,\n","                out_channels=out_channels,\n","                stride=1,\n","                weight_initializer=weight_initializer,\n","                activation_func=activation_func,\n","                use_batch_norm=use_batch_norm,\n","                use_instance_norm=use_instance_norm\n","            )\n","            layers.append(conv)\n","\n","        layers.append(torch.nn.MaxPool2d(kernel_size=2, stride=stride))\n","\n","        self.conv_block = torch.nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        '''\n","        Forward input x through a VGG block\n","\n","        Arg(s):\n","            x : torch.Tensor[float32]\n","                N x C x H x W input tensor\n","        Returns:\n","            torch.Tensor[float32] : N x K x h x w output tensor\n","        '''\n","\n","        return self.conv_block(x)"]}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}