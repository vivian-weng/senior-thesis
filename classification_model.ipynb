{"cells":[{"cell_type":"code","execution_count":null,"id":"3aa0b6d0","metadata":{"id":"3aa0b6d0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713718597351,"user_tz":240,"elapsed":34716,"user":{"displayName":"Vivian Weng","userId":"16986109935608411488"}},"outputId":"8c0b0cbb-d2ee-4f3c-8729-8b6a3de6c6f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Mounted at /content/drive\n"]}],"source":["import torch\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","%run '/content/drive/MyDrive/Colab Notebooks/CPSC 490/model/log_utils.ipynb'\n","%run '/content/drive/MyDrive/Colab Notebooks/CPSC 490/model/networks.ipynb'"]},{"cell_type":"code","execution_count":null,"id":"98d3a6b1","metadata":{"comment_questions":false,"id":"98d3a6b1","colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"status":"error","timestamp":1714418446307,"user_tz":240,"elapsed":935,"user":{"displayName":"Vivian Weng","userId":"16986109935608411488"}},"outputId":"8aa28324-4854-4e59-fbd1-f1bfcddbb825"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'torch' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-ec5de6e751dd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mClassificationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     '''\n\u001b[1;32m      3\u001b[0m     \u001b[0mClassification\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mthat\u001b[0m \u001b[0msupports\u001b[0m \u001b[0mVGG11\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mResNet18\u001b[0m \u001b[0mencoders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mArg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-ec5de6e751dd>\u001b[0m in \u001b[0;36mClassificationModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     def __init__(self,\n\u001b[1;32m     13\u001b[0m                  \u001b[0mencoder_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                  device=torch.device('cuda')):\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}],"source":["class ClassificationModel(object):\n","    '''\n","    Classification model class that supports VGG11 and ResNet18 encoders\n","\n","    Arg(s):\n","        encoder_type : str\n","            encoder options to build: vggnet11, resnet18, etc.\n","        device : torch.device\n","            device to run model on\n","    '''\n","\n","    def __init__(self,\n","                 encoder_type,\n","                 device=torch.device('cuda')):\n","\n","        self.device = device\n","\n","        # TODO: Instantiate VGG11 and ResNet18 encoders and decoders based on\n","        # https://arxiv.org/pdf/1409.1556.pdf\n","        # https://arxiv.org/pdf/1512.03385.pdf\n","        # Decoder should use\n","        # https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n","        input_channels = 3\n","        if encoder_type == 'vggnet11':\n","            self.encoder = VGGNet11Encoder(input_channels=input_channels,\n","                                           n_filters=[64, 128, 256, 512, 512])\n","            self.decoder = torch.nn.Sequential(torch.nn.Flatten(),\n","                                               torch.nn.Linear(512, 4096),\n","                                               torch.nn.ReLU(True),\n","                                               torch.nn.Dropout(),\n","                                               torch.nn.Linear(4096, 4096),\n","                                               torch.nn.ReLU(True),\n","                                               torch.nn.Dropout(),\n","                                               torch.nn.Linear(4096, 1000))\n","        elif encoder_type == 'resnet18':\n","            self.encoder = ResNet18Encoder(input_channels=input_channels,\n","                                           n_filters=[64, 64, 128, 256, 512])\n","            self.decoder = torch.nn.Sequential(torch.nn.AdaptiveAvgPool2d((1,1)),\n","                                              torch.nn.Flatten(1, -1),\n","                                              torch.nn.Linear(512, 10))\n","        else:\n","            raise ValueError('Unsupported encoder type: {}'.format(encoder_type))\n","        self.encoder_type = encoder_type\n","\n","    def transform_input(self, images):\n","        '''\n","        Transforms input based on model arguments and settings\n","\n","        Arg(s):\n","            images : torch.Tensor[float32]\n","                N x C x H x W images\n","        Returns:\n","            torch.Tensor[float32] : transformed N x C x H x W images\n","        '''\n","\n","        # TODO: Perform normalization based on\n","        # https://arxiv.org/pdf/1409.1556.pdf\n","        # https://arxiv.org/pdf/1512.03385.pdf\n","\n","        if self.encoder_type == 'vggnet11':\n","            images = images / 255.0\n","        elif self.encoder_type == 'resnet18':\n","            normalize = torch.nn.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","            images = normalize(images)\n","\n","        return images\n","\n","    def forward(self, image):\n","        '''\n","        Forwards inputs through the network\n","\n","        Arg(s):\n","            image : torch.Tensor[float32]\n","                N x 3 x H x W image\n","        Returns:\n","            torch.Tensor[float32] : N x K predicted class confidences\n","        '''\n","\n","        # TODO: Implement forward function\n","        latent_space, intermediate = self.encoder(image)\n","        output = self.decoder(latent_space)\n","\n","        return output\n","\n","    def compute_loss(self, output, label):\n","        '''\n","        Compute cross entropy loss\n","\n","        Arg(s):\n","            output : torch.Tensor[float32]\n","                N x K predicted class confidences\n","            label : torch.Tensor[int]\n","                ground truth class labels\n","        Returns:\n","            float : loss averaged over the batch\n","            dict[str, float] : dictionary of loss related tensors\n","        '''\n","\n","        # TODO: Compute cross entropy loss\n","        loss = torch.nn.functional.cross_entropy(\n","            input=output,\n","            target=label)\n","\n","        loss_info = {\n","            'loss' : loss.item()\n","        }\n","\n","        return loss, loss_info\n","\n","    def parameters(self):\n","        '''\n","        Returns the list of parameters in the model\n","\n","        Returns:\n","            list[torch.Tensor[float32]] : list of parameters\n","        '''\n","\n","        return list(self.encoder.parameters()) + list(self.decoder.parameters())\n","\n","    def train(self):\n","        '''\n","        Sets model to training mode\n","        '''\n","        self.encoder.train()\n","        self.decoder.train()\n","\n","    def eval(self):\n","        '''\n","        Sets model to evaluation mode\n","        '''\n","\n","        self.encoder.eval()\n","        self.decoder.eval()\n","\n","    def to(self, device):\n","        '''\n","        Move model to a device\n","\n","        Arg(s):\n","            device : torch.device\n","                device to use\n","        '''\n","\n","        self.device = device\n","\n","        self.encoder.to(self.device)\n","        self.decoder.to(self.device)\n","\n","        # TODO: Move encoder and decoder to device\n","\n","    def data_parallel(self):\n","        '''\n","        Allows multi-gpu split along batch\n","        '''\n","\n","        # TODO: Wrap encoder and decoder in\n","        # https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html\n","\n","        self.encoder = torch.nn.DataParallel(self.encoder)\n","        self.decoder = torch.nn.DataParallel(self.decoder)\n","\n","    def restore_model(self, restore_path, optimizer=None):\n","        '''\n","        Loads weights from checkpoint\n","\n","        Arg(s):\n","            restore_path : str\n","                lists of paths to model weights\n","            optimizer : torch.optim or None\n","                current optimizer\n","        Returns:\n","            int : training step\n","            torch.optim : restored optimizer or None if no optimizer is passed in\n","        '''\n","\n","        # TODO: Restore the weights from checkpoint\n","        # Encoder and decoder are keyed using 'encoder_state_dict' and 'decoder_state_dict'\n","        # If optimizer is given, then save its parameters using key 'optimizer_state_dict'\n","        checkpoint = torch.load(restore_path)\n","        self.encoder.load_state_dict(checkpoint['encoder_state_dict'])\n","        self.decoder.load_state_dict(checkpoint['decoder_state_dict'])\n","\n","        if optimizer:\n","            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","        return checkpoint['step'], optimizer\n","\n","    def save_model(self, checkpoint_path, step, optimizer=None):\n","        '''\n","        Save weights of the model to checkpoint path\n","\n","        Arg(s):\n","            checkpoint_path : str\n","                list path to save checkpoint\n","            step : int\n","                current training step\n","            optimizer : torch.optim\n","                optimizer\n","        '''\n","\n","        # TODO: Save the weights into checkpoint\n","        # Encoder and decoder are keyed using 'encoder_state_dict' and 'decoder_state_dict'\n","        # If optimizer is given, then save its parameters using key 'optimizer_state_dict'\n","        torch.save({\n","            'step': step,\n","            'encoder_state_dict': self.encoder.state_dict(),\n","            'decoder_state_dict': self.decoder.state_dict(),\n","            'optimzier_state_dict': optimizer.state_dict() if optimizer else None\n","        },\n","        checkpoint_path)\n","\n","    def log_summary(self,\n","                    summary_writer,\n","                    tag,\n","                    step,\n","                    image,\n","                    output,\n","                    ground_truth,\n","                    scalars={},\n","                    n_image_per_summary=16):\n","        '''\n","        Logs summary to Tensorboard\n","\n","        Arg(s):\n","            summary_writer : SummaryWriter\n","                Tensorboard summary writer\n","            tag : str\n","                tag that prefixes names to log\n","            step : int\n","                current step in training\n","            image : torch.Tensor[float32] 640 x 480\n","                image at time step\n","            output : torch.Tensor[float32]\n","                N\n","            label : torch.Tensor[float32]\n","                ground truth force measurements or ground truth bounding box and force measurements\n","            scalars : dict[str, float]\n","                dictionary of scalars to log\n","            n_image_per_summary : int\n","                number of images to display\n","        '''\n","\n","        grid_size = int(np.sqrt(n_image_per_summary))\n","        n_image_per_summary = int(grid_size * grid_size)\n","\n","        with torch.no_grad():\n","\n","            image_summary = image[0:n_image_per_summary, ...] / 255.0\n","\n","            # TODO: Move image_summary to CPU using cpu()\n","            image_summary = image_summary.cpu()\n","\n","            # TODO: Convert image_summary to numpy using numpy() and swap dimensions from NCHW to NHWC\n","            image_summary = image_summary.numpy().transpose((0,2,3,1))\n","\n","            # TODO: Create plot figure of size n x n using log_utils\n","            grid_size = int(n_image_per_summary ** 0.5)\n","            images_display = []\n","            subplot_titles = []\n","            for row in range(grid_size):\n","                idx_start = row * grid_size\n","                idx_end = (row+1) * grid_size\n","                images_display.append(image_summary[idx_start:idx_end])\n","                subplot_titles.append([f'output={output[i]}\\nlabel={ground_truth[i]}' for i in range(idx_start, idx_end)])\n","            figure = plot_images(images_display, grid_size, grid_size, subplot_titles)\n","\n","            # TODO: Log image summary to Tensorboard with <tag>_image as its summary tag name using\n","            # https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_figure\n","            summary_writer.add_figure(f'{tag}_image', figure, global_step=step)\n","\n","            plt.tight_layout()\n","\n","            plt.cla()\n","            plt.clf()\n","            plt.close()\n","\n","            # TODO: Log scalars to Tensorboard with <tag>_<name> as its summary tag name using\n","            # https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_scalar\n","            for (name, value) in scalars.items():\n","                summary_writer.add_scalar(f'{tag}_{name}', value, step)"]}],"metadata":{"jupytext":{"cell_metadata_filter":"comment_questions,-all","main_language":"python","notebook_metadata_filter":"-all"},"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}